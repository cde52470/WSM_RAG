diff --git a/My_RAG/chunker.py b/My_RAG/chunker.py
index 6840e98..71a8bfc 100644
--- a/My_RAG/chunker.py
+++ b/My_RAG/chunker.py
@@ -1,23 +1,144 @@
-def chunk_documents(docs, language, chunk_size=1000, chunk_overlap=200):
-    chunks = []
+import re
+from typing import Any, Dict, List, Optional
+
+def _validate_chunk_params(chunk_size: int, chunk_overlap: int) -> int:
+    """Validate chunk configuration."""
+    if chunk_size <= 0:
+        raise ValueError("chunk_size must be a positive integer.")
+    if chunk_overlap < 0:
+        raise ValueError("chunk_overlap cannot be negative.")
+    if chunk_overlap >= chunk_size:
+        raise ValueError("chunk_overlap must be smaller than chunk_size.")
+    return chunk_size - chunk_overlap
+
+def _split_text_into_sentences(text: str) -> List[Dict[str, Any]]:
+    """
+    使用正則表達式將文本切分為句子，並保留每個句子的起始位置。
+    支援中文 (。！？) 與英文 (.!?) 及換行符。
+    """
+    # 說明：
+    # [^。！？.!?\n]+  -> 匹配非標點符號的連續字元
+    # [。！？.!?\n]* -> 匹配跟隨在後的標點符號 (包含換行)
+    # 這樣可以確保標點符號會黏在句子後面，不會被切掉
+    sentence_pattern = re.compile(r'([^。！？.!?\n]+[。！？.!?\n]*)')
+    
+    sentences = []
+    # 使用 finditer 可以直接取得匹配文字的 start 和 end 索引
+    for match in sentence_pattern.finditer(text):
+        sentences.append({
+            "text": match.group(),
+            "start": match.start(),
+            "end": match.end()
+        })
+    
+    # 如果文本沒有標點符號 (例如只有一句話)，直接回傳整個文本
+    if not sentences and text:
+        sentences.append({"text": text, "start": 0, "end": len(text)})
+        
+    return sentences
+
+def chunk_documents(
+    docs: List[Dict[str, Any]],
+    language: Optional[str] = None,
+    chunk_size: int = 1000,
+    chunk_overlap: int = 200,
+) -> List[Dict[str, Any]]:
+    """
+    優化版的切分函數：基於句子邊界進行切分，避免切斷語意。
+    """
+    _validate_chunk_params(chunk_size, chunk_overlap)
+    chunks: List[Dict[str, Any]] = []
+
     for doc_index, doc in enumerate(docs):
-        if 'content' in doc and isinstance(doc['content'], str) and 'language' in doc:
-            text = doc['content']
-            text_len = len(text)
-            lang = doc['language']
-            start_index = 0
-            chunk_count = 0
-            if lang == language:
-                while start_index < text_len:
-                    end_index = min(start_index + chunk_size, text_len)
-                    chunk_metadata = doc.copy()
-                    chunk_metadata.pop('content', None)
-                    chunk_metadata['chunk_index'] = chunk_count
-                    chunk = {
-                        'page_content': text[start_index:end_index],
-                        'metadata': chunk_metadata
-                    }
-                    chunks.append(chunk)
-                    start_index += chunk_size - chunk_overlap
-                    chunk_count += 1
-    return chunks
+        text = doc.get("content")
+        lang = doc.get("language")
+
+        # 1. 基礎檢查與過濾
+        if not isinstance(text, str) or not text.strip():
+            continue
+        if language and lang != language:
+            continue
+
+        text = text.strip()
+        
+        # 2. 先將整篇文章切成「句子」清單
+        sentences = _split_text_into_sentences(text)
+        
+        current_chunk_sentences = []
+        current_chunk_len = 0
+        chunk_count = 0
+        
+        i = 0
+        while i < len(sentences):
+            sent = sentences[i]
+            sent_len = len(sent["text"])
+            
+            # 3. 判斷是否加入當前 Chunk
+            # 如果加入這句會爆掉 chunk_size，且當前 chunk 不為空，則先結算當前 chunk
+            if current_chunk_len + sent_len > chunk_size and current_chunk_sentences:
+                # --- 結算目前的 Chunk ---
+                chunk_text = "".join([s["text"] for s in current_chunk_sentences])
+                
+                # 複製原始 Metadata
+                chunk_metadata = {k: v for k, v in doc.items() if k != "content"}
+                chunk_metadata.update({
+                    "chunk_index": chunk_count,
+                    "doc_index": doc_index,
+                    "char_start": current_chunk_sentences[0]["start"],
+                    "char_end": current_chunk_sentences[-1]["end"],
+                })
+
+                chunks.append({
+                    "page_content": chunk_text,
+                    "metadata": chunk_metadata
+                })
+                chunk_count += 1
+                
+                # --- 處理 Overlap (回溯) ---
+                # 我們需要保留尾部的句子作為下一個 Chunk 的開頭 (Overlap)
+                # 從後面往回算，直到湊滿 chunk_overlap 的長度
+                overlap_len = 0
+                new_start_idx = i  # 預設不重疊 (如果 overlap 設很小)
+                
+                for k in range(len(current_chunk_sentences) - 1, -1, -1):
+                    overlap_len += len(current_chunk_sentences[k]["text"])
+                    if overlap_len > chunk_overlap:
+                        # 找到剛好超過 overlap 的點，這就是下一句的起點
+                        new_start_idx = i - (len(current_chunk_sentences) - k) + 1
+                        # 修正：確保至少會前進一步，避免無窮迴圈
+                        new_start_idx = max(new_start_idx, i - len(current_chunk_sentences) + 1)
+                        break
+                
+                # 如果 overlap 沒填滿整個 current chunk，我們就從計算出的位置重新開始
+                # 如果 overlap 比整段還長 (罕見)，就只退一步
+                if chunk_overlap > 0:
+                    i = new_start_idx
+                
+                # 重置 Buffer
+                current_chunk_sentences = []
+                current_chunk_len = 0
+                
+                # 注意：這裡不 i+=1，因為我們要用新的 i 重新跑迴圈來判定當前句子
+                continue
+
+            # 4. 加入句子到 Buffer
+            current_chunk_sentences.append(sent)
+            current_chunk_len += sent_len
+            i += 1
+        
+        # 5. 處理最後一個剩下的 Chunk
+        if current_chunk_sentences:
+            chunk_text = "".join([s["text"] for s in current_chunk_sentences])
+            chunk_metadata = {k: v for k, v in doc.items() if k != "content"}
+            chunk_metadata.update({
+                "chunk_index": chunk_count,
+                "doc_index": doc_index,
+                "char_start": current_chunk_sentences[0]["start"],
+                "char_end": current_chunk_sentences[-1]["end"],
+            })
+            chunks.append({
+                "page_content": chunk_text,
+                "metadata": chunk_metadata
+            })
+
+    return chunks
\ No newline at end of file
diff --git a/My_RAG/generator.py b/My_RAG/generator.py
index 953fe9d..a35e8d1 100644
--- a/My_RAG/generator.py
+++ b/My_RAG/generator.py
@@ -1,53 +1,116 @@
 from ollama import Client
-import os
+from pathlib import Path
+import yaml
+import re
+
+def load_ollama_config() -> dict:
+    # (維持原本的 Config 讀取邏輯不變)
+    configs_folder = Path(__file__).parent.parent / "configs"
+    config_paths = [
+        configs_folder / "config_local.yaml",
+        configs_folder / "config_submit.yaml",
+    ]
+    config_path = None
+    for path in config_paths:
+        if path.exists():
+            config_path = path
+            break
+
+    if config_path is None:
+        return {"host": "http://ollama-gateway:11434", "model": "granite4:3b"}
+
+    with open(config_path, "r") as file:
+        config = yaml.safe_load(file)
+
+    return config.get("ollama", {})
+
+def is_contains_chinese(strs):
+    """檢查字串是否包含中文字元"""
+    for _char in strs:
+        if '\u4e00' <= _char <= '\u9fff':
+            return True
+    return False
+
+def _parse_model_output(response_text: str, language: str) -> str:
+    """
+    解析模型輸出，移除思考過程，只保留最終答案。
+    """
+    # 定義要捕捉的標籤
+    tags = ["Answer:", "最終答案：", "Final Answer:", "回答："]
+    
+    # 1. 嘗試尋找分割點
+    content = response_text.strip()
+    for tag in tags:
+        if tag in content:
+            # 取 tag 之後的所有文字
+            content = content.split(tag)[-1].strip()
+            return content
+            
+    # 2. 如果沒找到 tag (模型沒乖乖聽話)，嘗試用換行符號猜測
+    # 通常思考過程長，答案短，或是思考在第一段。
+    # 這裡採取保守策略：如果沒 tag，就回傳全部，避免切錯。
+    return content
 
 def generate_answer(query, context_chunks):
+    # 1. 準備 Context
     context = "\n\n".join([chunk['page_content'] for chunk in context_chunks])
-    prompt = f"""You are a helpful assistant for question-answering tasks.
-Use the following pieces of retrieved context to answer the question.
-If the answer is not in the context, just say that you don't know.
-Keep the answer concise and strictly based on the provided context.
-
-<context>
-{context}
-</context>
-
-<question>
-{query}
-</question>
-
-Answer:"""
-    hosts_to_try = [
-        "http://ollama-gateway:11434",  # Submission host
-        "http://ollama:11434",          # Local Docker host
-        "http://localhost:11434"        # Local Conda host
-    ]
-    model_name = os.getenv("GENERATOR_MODEL", "granite4:3b")
-    last_error = None
-
-    for host in hosts_to_try:
-        try:
-            client = Client(host=host)
-            # Use a lightweight call to check for connectivity before generating
-            client.list()
-            # If connectivity is confirmed, proceed with generation
-            response = client.generate(model=model_name, prompt=prompt, stream=False)
-            return response.get("response", "No response from model.")
-        except Exception as e:
-            # print(f"Info: Failed to connect to {host}. Trying next host. Error: {e}")
-            last_error = e
-            continue  # Try the next host in the list
-
-    # If all hosts fail, return the last error
-    return f"Error: Could not connect to any Ollama host. Last error: {last_error}"
+    
+    # 2. 準備 Prompt (加入 CoT 與格式要求)
+    if is_contains_chinese(query):
+        # 【中文 Prompt：強調推論與格式】
+        prompt = (
+            "你是一個嚴謹的問答助手。請僅根據提供的「參考內容」回答問題。\n"
+            "若參考內容中沒有答案，請直接說「我不知道」，不可編造。\n\n"
+            "請嚴格遵守以下輸出格式：\n"
+            "思考過程：<請在此簡短分析參考內容與問題的關聯>\n"
+            "最終答案：<請在此給出最終的繁體中文回答，不超過三句話>\n\n"
+            f"參考內容 (Context):\n{context}\n\n"
+            f"使用者問題 (Question): {query}\n"
+        )
+    else:
+        # 【英文 Prompt：強調 Reasoning 與 Format】
+        prompt = (
+            "You are a strict assistant. Answer the question based ONLY on the provided context.\n"
+            "If the answer is not in the context, say 'I don't know'. Do not hallucinate.\n\n"
+            "Please follow this format strictly:\n"
+            "Thinking: <Briefly analyze the context and reasoning here>\n"
+            "Answer: <Provide the final concise answer here, max 3 sentences>\n\n"
+            f"Context:\n{context}\n\n"
+            f"Question: {query}\n"
+        )
 
+    # 3. 呼叫模型
+    ollama_config = load_ollama_config()
+    # 優先使用 config 設定，若無則 fallback 到預設值
+    host = ollama_config.get("host", "http://localhost:11434")
+    # 建議之後換成 qwen2.5:3b 以獲得更好的中文效果
+    model = "granite4:3b" # 或者保留原本的 "granite4:3b"
+    
+    try:
+        client = Client(host=host)
+        response = client.generate(model=model, prompt=prompt)
+        raw_output = response["response"]
+        
+        # 4. 解析輸出 (只回傳 Answer 部分)
+        final_answer = _parse_model_output(raw_output, "zh" if is_contains_chinese(query) else "en")
+        return final_answer
+        
+    except Exception as e:
+        print(f"Generate Error: {e}")
+        return "Sorry, generation failed."
 
 if __name__ == "__main__":
-    # test the function
-    query = "What is the capital of France?"
-    context_chunks = [
-        {"page_content": "France is a country in Europe. Its capital is Paris."},
-        {"page_content": "The Eiffel Tower is located in Paris, the capital city of France."}
+    # 測試程式
+    query_zh = "法國的首都在哪裡？"
+    chunks = [
+        {"page_content": "法國（France），全名法蘭西共和國。"},
+        {"page_content": "巴黎（Paris）是法國的首都及最大都市。"}
     ]
-    answer = generate_answer(query, context_chunks)
-    print("Generated Answer:", answer)
\ No newline at end of file
+    
+    print("Testing Chinese Query...")
+    ans = generate_answer(query_zh, chunks)
+    print(f"Parsed Answer: {ans}")
+    
+    print("\nTesting English Query...")
+    ans_en = generate_answer("What is the capital of France?", chunks)
+    print(f"Parsed Answer: {ans_en}")
\ No newline at end of file
diff --git a/My_RAG/main.py b/My_RAG/main.py
index 774ff56..c5a5376 100644
--- a/My_RAG/main.py
+++ b/My_RAG/main.py
@@ -28,7 +28,7 @@ def main(query_path, docs_path, language, output_path):
         # 4. Retrieve relevant chunks
         query_text = query['query']['content']
         # print(f"\nRetrieving chunks for query: '{query_text}'")
-        retrieved_chunks = retriever.retrieve(query_text)
+        retrieved_chunks = retriever.retrieve(query_text, top_k=3, use_hyde=True)
         # print(f"Retrieved {len(retrieved_chunks)} chunks.")
 
         # 5. Generate Answer
diff --git a/My_RAG/retriever.py b/My_RAG/retriever.py
index 33092cd..68b3ce1 100644
--- a/My_RAG/retriever.py
+++ b/My_RAG/retriever.py
@@ -1,25 +1,174 @@
-from rank_bm25 import BM25Okapi
+from typing import Any, Dict, List, Optional, Tuple
+import numpy as np
 import jieba
+from rank_bm25 import BM25Okapi
+from ollama import Client
+from generator import load_ollama_config
+
+# 移除原本的 Ollama Client，因為 Rerank 不建議用生成式模型
+try:
+    from sentence_transformers import SentenceTransformer, CrossEncoder
+except ImportError:
+    SentenceTransformer = None
+    CrossEncoder = None
 
-class BM25Retriever:
-    def __init__(self, chunks, language="en"):
-        self.chunks = chunks
+class HybridRetriever:
+    def __init__(
+        self,
+        chunks: List[Dict[str, Any]],
+        language: str = "en",
+        bm25_top_k: int = 50,      # BM25 取前 50
+        embed_top_k: int = 50,     # 向量取前 50
+        final_top_k: int = 5,      # 最終回傳前 5
+        # rerank_model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2", # 推薦的輕量級 Reranker
+        rerank_model_name: str = "BAAI/bge-reranker-base", # 將預設模型換成支援中英雙語的 BAAI/bge-reranker-base
+        embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
+        use_reranker: bool = True
+    ):
         self.language = language
-        self.corpus = [chunk['page_content'] for chunk in chunks]
-        if language == "zh":
-            self.tokenized_corpus = [list(jieba.cut(doc)) for doc in self.corpus]
-        else:
-            self.tokenized_corpus = [doc.split(" ") for doc in self.corpus]
+        # 1. 過濾語言
+        self.chunks = [
+            c for c in chunks
+            if not language
+            or c.get("metadata", {}).get("language") == language
+            or c.get("language") == language
+        ]
+        self.corpus = [chunk["page_content"] for chunk in self.chunks]
+        
+        self.bm25_top_k = bm25_top_k
+        self.embed_top_k = embed_top_k
+        self.final_top_k = final_top_k
+        self.use_reranker = use_reranker
+
+        # 2. 初始化 BM25
+        self.tokenized_corpus = [self._tokenize(doc) for doc in self.corpus]
         self.bm25 = BM25Okapi(self.tokenized_corpus)
 
-    def retrieve(self, query, top_k=5):
+        # 3. 初始化 Embedding Model (Bi-Encoder)
+        self.dense_encoder = None
+        self.chunk_embeddings = None
+        if SentenceTransformer:
+            self.dense_encoder = SentenceTransformer(embedding_model_name)
+            # 預計算所有 chunks 的向量 (實務上這步應該在存資料庫時就做好了，不要每次 init 做)
+            # 這裡使用 encode(show_progress_bar=False) 且直接轉 numpy，比迴圈快
+            self.chunk_embeddings = self.dense_encoder.encode(
+                self.corpus, convert_to_numpy=True, normalize_embeddings=True
+            )
+
+        # 4. 初始化 Reranker (Cross-Encoder)
+        self.reranker = None
+        if CrossEncoder and use_reranker:
+            # 這是一個專門用來評分 (Query, Document) 相關性的模型，速度極快
+            self.reranker = CrossEncoder(rerank_model_name)
+
+        try:
+            config = load_ollama_config()
+            ollama_host = config.get("host", "http://ollama-gateway:11434")
+        except Exception:
+            ollama_host = "http://ollama-gateway:11434"
+            
+        self.ollama_client = Client(host=ollama_host)
+
+    def _tokenize(self, text: str):
         if self.language == "zh":
-            tokenized_query = list(jieba.cut(query))
+            return list(jieba.cut(text))
+        return text.split()
+
+    def _rrf_fusion(self, bm25_indices: List[int], embed_indices: List[int], k: int = 60) -> List[int]:
+        """
+        Reciprocal Rank Fusion (RRF):
+        不依賴絕對分數，而是依賴「排名」。解決了 BM25 分數和 Cosine 分數範圍不同的問題。
+        """
+        rrf_score = {}
+
+        # 處理 BM25 排名
+        for rank, idx in enumerate(bm25_indices):
+            if idx not in rrf_score: rrf_score[idx] = 0
+            rrf_score[idx] += 1 / (k + rank + 1)
+
+        # 處理 Vector 排名
+        for rank, idx in enumerate(embed_indices):
+            if idx not in rrf_score: rrf_score[idx] = 0
+            rrf_score[idx] += 1 / (k + rank + 1)
+
+        # 根據 RRF 分數排序，由高到低
+        sorted_indices = sorted(rrf_score.items(), key=lambda x: x[1], reverse=True)
+        return [idx for idx, score in sorted_indices]
+
+    def _generate_hyde_doc(self, query: str) -> str:
+        """
+        使用 LLM 生成一個假設性的答案 (Hypothetical Document)
+        """
+        prompt = (
+            f"Please write a passage to answer the question\n"
+            f"Question: {query}\n"
+            f"Passage:"
+        )
+        try:
+            # 這裡用一個比較快的小模型生成即可，例如 llama3:8b 或 gemma
+            response = self.ollama_client.generate(model = "granite4:3b", prompt=prompt)
+            return response.get("response", "").strip()
+        except Exception as e:
+            print(f"HyDE generation failed: {e}")
+            return query  # 如果生成失敗，退回使用原始 query
+
+    def retrieve(self, query: str, top_k: int = None, use_hyde: bool = False) -> List[Dict[str, Any]]:
+        final_k = top_k if top_k is not None else self.final_top_k
+        
+        # --- 1. 決定用於向量檢索的文字 ---
+        search_text_for_vector = query
+        if use_hyde and self.dense_encoder:
+            # 如果開啟 HyDE，先生成假答案，用假答案去轉向量
+            fake_doc = self._generate_hyde_doc(query)
+            # print(f"HyDE Generated: {fake_doc[:50]}...") # debug用
+            search_text_for_vector = fake_doc
+        
+        query_tokens = self._tokenize(query)
+        
+        # --- 階段 1: BM25 檢索 ---
+        # get_scores 比較快，不要用 get_top_n
+        bm25_scores = self.bm25.get_scores(query_tokens)
+        # 取得前 N 名的 index
+        bm25_top_indices = np.argsort(bm25_scores)[::-1][:self.bm25_top_k]
+
+        # --- 階段 2: 向量檢索 (Vector Search) ---
+        embed_top_indices = []
+        if self.dense_encoder and self.chunk_embeddings is not None:
+            query_embedding = self.dense_encoder.encode(query, convert_to_numpy=True, normalize_embeddings=True)
+            
+            # 使用矩陣運算一次計算所有相似度 (Dot Product 因為已 Normalize = Cosine Similarity)
+            # 這是 Numpy 的廣播機制，比 for loop 快非常多
+            similarities = np.dot(self.chunk_embeddings, query_embedding)
+            embed_top_indices = np.argsort(similarities)[::-1][:self.embed_top_k]
+
+        # --- 階段 3: 融合 (Hybrid Fusion) ---
+        # 使用 RRF 合併兩種結果，取前 N 個候選人進入 Rerank
+        merged_indices = self._rrf_fusion(bm25_top_indices, embed_top_indices)
+        
+        # 這裡的候選集數量可以稍微多一點，例如取前 50 個給 Reranker
+        candidate_indices = merged_indices[:50] 
+        candidate_docs = [self.corpus[i] for i in candidate_indices]
+
+        # --- 階段 4: 重排序 (Re-ranking) ---
+        if self.reranker:
+            # Cross-Encoder 接受 list of pairs: [(query, doc1), (query, doc2), ...]
+            pairs = [[query, doc] for doc in candidate_docs]
+            rerank_scores = self.reranker.predict(pairs)
+            
+            # 結合 index 和分數
+            results_with_scores = list(zip(candidate_indices, rerank_scores))
+            # 根據 rerank 分數重新排序
+            results_with_scores.sort(key=lambda x: x[1], reverse=True)
+            
+            final_indices = [idx for idx, score in results_with_scores[:self.final_top_k]]
         else:
-            tokenized_query = query.split(" ")
-        top_chunks = self.bm25.get_top_n(tokenized_query, self.chunks, n=top_k)
-        return top_chunks
+            # 如果沒有 Reranker，直接回傳 RRF 的結果
+            final_indices = candidate_indices[:self.final_top_k]
+
+
+
+        return [self.chunks[idx] for idx in final_indices]
 
-def create_retriever(chunks, language):
-    """Creates a BM25 retriever from document chunks."""
-    return BM25Retriever(chunks, language)
+# 使用範例
+def create_retriever(chunks, language="en"):
+    return HybridRetriever(chunks, language=language)
\ No newline at end of file
diff --git a/My_RAG/utils.py b/My_RAG/utils.py
index 47573f4..6609d26 100644
--- a/My_RAG/utils.py
+++ b/My_RAG/utils.py
@@ -1,4 +1,5 @@
 import jsonlines
+from pathlib import Path
 
 def load_jsonl(file_path):
     docs = []
@@ -8,6 +9,7 @@ def load_jsonl(file_path):
     return docs
 
 def save_jsonl(file_path, data):
+    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
     with jsonlines.open(file_path, mode='w') as writer:
         for item in data:
             writer.write(item)
\ No newline at end of file
